{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b03688-cad1-4b47-b9d5-cfc27f31a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切换工作目录\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "target_dir_name = 'MimicMotion'\n",
    "\n",
    "target_dir = os.path.join(current_dir, target_dir_name)\n",
    "\n",
    "if not current_dir.endswith(target_dir_name):\n",
    "    if os.path.exists(target_dir):\n",
    "        os.chdir(target_dir)\n",
    "        print(f\"Current working directory changed to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(f\"Target directory {target_dir} does not exist.\")\n",
    "else:\n",
    "    print(f\"Already in the target directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8877b84-7951-41e5-bafb-7b2f7cd4056e",
   "metadata": {},
   "source": [
    "# 优化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c6157c-ea15-40b8-8a64-e131271f4028",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. 处理视频数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef941ef-c998-4464-b6c1-2589cc867232",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import ast\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from config import mimicmotion_root\n",
    "\n",
    "def load():\n",
    "    paths = [mimicmotion_root, os.path.join(mimicmotion_root, 'MeshGraphormer'),\\\n",
    "             os.path.join(mimicmotion_root, 'mimicmotion', 'modules'),\\\n",
    "             os.path.join(mimicmotion_root, 'dataset')]\n",
    "    for p in paths:\n",
    "        sys.path.insert(0, p)\n",
    "load()\n",
    "\n",
    "import mediapipe as mp\n",
    "# from mediapipe.tasks.python.vision import ImageFormat\n",
    "from controlnet_aux.util import HWC3, resize_image\n",
    "\n",
    "from mimicmotion.modules.meshgraphormer import MeshGraphormerMediapipe\n",
    "from mimicmotion.dwpose.util import draw_bodypose, draw_handpose, draw_facepose\n",
    "from mimicmotion.utils.utils import get_fps, save_videos_from_pil, read_frames, read_handframes\n",
    "\n",
    "meshgraphormer = MeshGraphormerMediapipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f6d28-5f45-4ec9-bc4d-64f7296880a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_video_hand(video_path, detector, root_dir, save_dir, save_mask_dir, detect_resolution=576, image_resolution=512, output_type=\"pil\", padding_bbox=30):\n",
    "    relative_path = os.path.relpath(video_path, root_dir)\n",
    "    \n",
    "    out_path = os.path.join(save_dir, relative_path)\n",
    "    out_mask_path = os.path.join(save_mask_dir, relative_path)\n",
    "    print('relative_path, video_path, root_dir', relative_path, video_path, root_dir)\n",
    "    if os.path.exists(out_path) and os.path.exists(out_mask_path) :\n",
    "        return\n",
    "    \n",
    "    output_dir = Path(os.path.dirname(os.path.join(save_dir, relative_path)))\n",
    "    output_mask_dir = Path(os.path.dirname(os.path.join(save_mask_dir, relative_path)))\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    if not output_mask_dir.exists():\n",
    "        output_mask_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    fps = get_fps(video_path)\n",
    "    frames = read_frames(video_path)\n",
    "    kps_results = []\n",
    "    kps_mask_results = []\n",
    "    for i, frame_pil in enumerate(frames):\n",
    "        print(f\"Processing frame {i+1}\", end='\\r')\n",
    "        \n",
    "        # input_image = cv2.cvtColor(\n",
    "        #     np.array(frame_pil, dtype=np.uint8), cv2.COLOR_RGB2BGR\n",
    "        # )\n",
    "        input_image = np.array(frame_pil)\n",
    "        # input_image = HWC3(input_image)\n",
    "        # input_image = resize_image(input_image, detect_resolution)\n",
    "        # H, W, C = input_image.shape\n",
    "        \n",
    "        # input_image_mp = mp.Image(image_format=mp.ImageFormat.SRGB, data=input_image)\n",
    "        # input_image = convert_to_mp_image(frame_pil)\n",
    "        depthmap, mask, info = detector.get_hand(input_image, padding_bbox, relative_path)\n",
    "        \n",
    "        if depthmap is None:\n",
    "            depthmap = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "            depthmap = HWC3(depthmap)\n",
    "            mask = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "            mask = HWC3(mask)\n",
    "        else:\n",
    "            depthmap = HWC3(depthmap)\n",
    "            mask = HWC3(mask)\n",
    "\n",
    "        # depthmap = resize_image(depthmap, image_resolution)\n",
    "        H, W, C = depthmap.shape\n",
    "        depthmap = cv2.resize(depthmap, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # mask = resize_image(mask, image_resolution)\n",
    "        H, W, C = mask.shape\n",
    "        mask = cv2.resize(mask, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        if output_type == \"pil\":\n",
    "            depthmap = Image.fromarray(depthmap)\n",
    "            mask = Image.fromarray(mask)\n",
    "            \n",
    "        kps_results.append(depthmap)\n",
    "        kps_mask_results.append(mask)\n",
    "        \n",
    "    save_videos_from_pil(kps_results, out_path, fps=fps)\n",
    "    save_videos_from_pil(kps_mask_results, out_mask_path, fps=fps)\n",
    "\n",
    "def process_batch_videos_hand(video_list, detector, root_dir, save_dir, pose_mask_dir):\n",
    "    for i, video_path in enumerate(video_list):\n",
    "        print(f\"Process {i}/{len(video_list)} video\")\n",
    "        process_single_video_hand(video_path, detector, root_dir, save_dir, pose_mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ccce80-2a7d-49cb-8339-110cf4049c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_root_dir = \"assets/test_data/videos\"\n",
    "pose_save_dir = pose_root_dir + \"_dwhand\"\n",
    "pose_mask_dir = pose_root_dir + \"_dwmask\"\n",
    "\n",
    "pose_mp4_paths = set()\n",
    "for root, dirs, files in os.walk(pose_root_dir):\n",
    "    for name in files:\n",
    "        if name.endswith(\".mp4\"):\n",
    "            pose_mp4_paths.add(os.path.join(root, name))\n",
    "pose_mp4_paths = list(pose_mp4_paths)\n",
    "pose_mp4_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6a541-2fd8-4509-bbbd-aa29908d6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_batch_videos_hand(pose_mp4_paths, meshgraphormer, root_dir=pose_root_dir, save_dir=pose_save_dir,\\\n",
    "                         pose_mask_dir=pose_mask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a7db10-eea8-4e03-8fa1-9121f1696a90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. 处理参考图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3184e-a5ea-47b7-af9f-7b9dc16bfef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path, output_dir, weights, prompt, strength, seed):\n",
    "    start = time.time()\n",
    "    os.system(f'python handrefiner.py --input_img {image_path} --out_dir {output_dir} --strength {strength} --weights {weights} --prompt \"{prompt}\" --seed {seed}')\n",
    "    end = time.time()\n",
    "    diff = end - start\n",
    "    return diff\n",
    "\n",
    "import math\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms.functional import resize, center_crop, pil_to_tensor, to_pil_image\n",
    "\n",
    "def load_and_adjust_image(image_path, output_path, resolution=576, aspect_ratio=9/16):\n",
    "    # 从路径加载图像并转换为 RGB 格式\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_pixels = pil_to_tensor(image)  # 结果为形状 (C, H, W)\n",
    "    \n",
    "    # 获取原始图像的高度和宽度\n",
    "    h, w = image_pixels.shape[-2:]\n",
    "\n",
    "    # 根据图像的高度和宽度调整目标宽度和高度\n",
    "    if h > w:\n",
    "        w_target, h_target = resolution, int(resolution / aspect_ratio // 64) * 64\n",
    "    else:\n",
    "        w_target, h_target = int(resolution / aspect_ratio // 64) * 64, resolution\n",
    "\n",
    "    # 计算高宽比\n",
    "    h_w_ratio = float(h) / float(w)\n",
    "    \n",
    "    # 根据高宽比调整目标尺寸\n",
    "    if h_w_ratio < h_target / w_target:\n",
    "        h_resize, w_resize = h_target, math.ceil(h_target / h_w_ratio)\n",
    "    else:\n",
    "        h_resize, w_resize = math.ceil(w_target * h_w_ratio), w_target\n",
    "\n",
    "    # 调整图像尺寸并进行中心裁剪\n",
    "    image_pixels = resize(image_pixels, [h_resize, w_resize], antialias=True)\n",
    "    adjusted_image = center_crop(image_pixels, [h_target, w_target])\n",
    "    \n",
    "    adjusted_image_pil = to_pil_image(adjusted_image)\n",
    "    adjusted_image_pil.save(output_path)\n",
    "    \n",
    "    return adjusted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d11bcb-e063-4731-9585-048764b05344",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A woman smiles, her hands folded in front of her.\"\n",
    "weights = '../autodl-tmp/models/inpaint_depth_control.ckpt'\n",
    "strength = 0.55\n",
    "seed = 1\n",
    "\n",
    "file_path = 'assets/test_data/images/demo3.jpg'\n",
    "output_folder = 'assets/test_data/images'\n",
    "\n",
    "process_image(file_path, output_folder, weights, prompt, strength, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6295cc76-88a6-47b1-a29f-46fd6a2c28cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. 验证模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd545ecb-ff85-4ee2-a593-0d39ba48782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image_path = 'assets/test_data/images/demo1.jpg'\n",
    "ref_video_path = 'assets/test_data/videos/test1.mp4'\n",
    "ref_hand_path = 'assets/test_data/videos_dwhand/test1.mp4' \n",
    "ref_hand_mask_path = 'assets/test_data/videos_dwmask/test1.mp4' \n",
    "images_depth_path = 'assets/test_data/images_depth/demo1_depth.jpg' \n",
    "images_mask_path = 'assets/test_data/images_mask/demo1_mask.jpg' \n",
    "\n",
    "ref_image_path2 = 'assets/test_data/images/demo1.jpg'\n",
    "ref_video_path2 = 'assets/test_data/videos/test2.mp4'\n",
    "ref_hand_path2 = 'assets/test_data/videos_dwhand/test2.mp4' \n",
    "ref_hand_mask_path2 = 'assets/test_data/videos_dwmask/test2.mp4' \n",
    "images_depth_path2 = 'assets/test_data/images_depth/demo1_depth.jpg' \n",
    "images_mask_path2 = 'assets/test_data/images_mask/demo2_mask.jpg' \n",
    "\n",
    "num_frames = 16\n",
    "resolution = 576\n",
    "frames_overlap = 6\n",
    "num_inference_steps = 25\n",
    "noise_aug_strength = 0\n",
    "guidance_scale = 2.0\n",
    "sample_stride = 2\n",
    "fps = 15\n",
    "seed = 42\n",
    "\n",
    "conf = OmegaConf.create({\n",
    "    'base_model_path': 'models/SVD/stable-video-diffusion-img2vid-xt-1-1',\n",
    "    'ckpt_path': 'models/MimicMotion.pth',\n",
    "    'stage2_path': 'models/motion_module-2400.pth',\n",
    "    'test_case': [\n",
    "        {\n",
    "            'ref_video_path': ref_video_path,\n",
    "            'ref_image_path': ref_image_path,\n",
    "            'ref_hand_path': ref_hand_path,\n",
    "            'ref_hand_mask_path': ref_hand_mask_path,\n",
    "            'images_depth_path': images_depth_path,\n",
    "            'images_mask_path': images_mask_path,\n",
    "            'num_frames': num_frames,\n",
    "            'resolution': resolution,\n",
    "            'frames_overlap': frames_overlap,\n",
    "            'num_inference_steps': num_inference_steps,\n",
    "            'noise_aug_strength': noise_aug_strength,\n",
    "            'guidance_scale': guidance_scale,\n",
    "            'sample_stride': sample_stride,\n",
    "            'fps': fps,\n",
    "            'seed': seed,\n",
    "        },\n",
    "        {\n",
    "            'ref_video_path': ref_video_path2,\n",
    "            'ref_image_path': ref_image_path2,\n",
    "            'ref_hand_path': ref_hand_path2,\n",
    "            'ref_hand_mask_path': ref_hand_mask_path2,\n",
    "            'images_depth_path': images_depth_path2,\n",
    "            'images_mask_path': images_mask_path2,\n",
    "            'num_frames': num_frames,\n",
    "            'resolution': resolution,\n",
    "            'frames_overlap': frames_overlap,\n",
    "            'num_inference_steps': num_inference_steps,\n",
    "            'noise_aug_strength': noise_aug_strength,\n",
    "            'guidance_scale': guidance_scale,\n",
    "            'sample_stride': sample_stride,\n",
    "            'fps': fps,\n",
    "            'seed': seed,\n",
    "        },\n",
    "    ],\n",
    "})\n",
    "OmegaConf.save(conf, './configs/my_conf.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa730b-63e5-452b-8af8-bb54a3384786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行\n",
    "start = time.time()\n",
    "!python inference2.py --inference_config configs/my_conf.yaml\n",
    "end = time.time()\n",
    "\n",
    "diff = end - start\n",
    "if diff < 60:\n",
    "    print(f'耗时：{diff:.3f} 秒。')\n",
    "else:\n",
    "    print(f'耗时：{diff/60:.3f} 分。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c84ed-8f27-453a-a04e-823b290e1665",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 原始模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b5b18-d5d6-4030-a86f-15892e1e843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_video_path = 'assets/test_data/videos/test1.mp4'\n",
    "ref_image_path = 'assets/test_data/images/demo1.jpg'\n",
    "\n",
    "ref_video_path2 = 'assets/test_data/videos/test2.mp4'\n",
    "ref_image_path2 = 'assets/test_data/images/demo1.jpg'\n",
    "\n",
    "num_frames = 16\n",
    "resolution = 576\n",
    "frames_overlap = 6\n",
    "num_inference_steps = 25\n",
    "noise_aug_strength = 0\n",
    "guidance_scale = 2.0\n",
    "sample_stride = 2\n",
    "fps = 15\n",
    "seed = 42\n",
    "\n",
    "conf = OmegaConf.create({\n",
    "    'base_model_path': 'models/SVD/stable-video-diffusion-img2vid-xt-1-1',\n",
    "    'ckpt_path': 'models/MimicMotion.pth',\n",
    "    'test_case': [\n",
    "        {\n",
    "            'ref_video_path': ref_video_path,\n",
    "            'ref_image_path': ref_image_path,\n",
    "            'num_frames': num_frames,\n",
    "            'resolution': resolution,\n",
    "            'frames_overlap': frames_overlap,\n",
    "            'num_inference_steps': num_inference_steps,\n",
    "            'noise_aug_strength': noise_aug_strength,\n",
    "            'guidance_scale': guidance_scale,\n",
    "            'sample_stride': sample_stride,\n",
    "            'fps': fps,\n",
    "            'seed': seed,\n",
    "        },\n",
    "        {\n",
    "            'ref_video_path': ref_video_path2,\n",
    "            'ref_image_path': ref_image_path2,\n",
    "            'num_frames': num_frames,\n",
    "            'resolution': resolution,\n",
    "            'frames_overlap': frames_overlap,\n",
    "            'num_inference_steps': num_inference_steps,\n",
    "            'noise_aug_strength': noise_aug_strength,\n",
    "            'guidance_scale': guidance_scale,\n",
    "            'sample_stride': sample_stride,\n",
    "            'fps': fps,\n",
    "            'seed': seed,\n",
    "        },\n",
    "    ],\n",
    "})\n",
    "OmegaConf.save(conf, './configs/minic_conf.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0deb14a-ce54-49ac-9603-37d9a00ac528",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "!python inference.py --inference_config configs/minic_conf.yaml\n",
    "end = time.time()\n",
    "\n",
    "diff = end - start\n",
    "if diff < 60:\n",
    "    print(f'耗时：{diff:.3f} 秒。')\n",
    "else:\n",
    "    print(f'耗时：{diff/60:.3f} 分。')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
